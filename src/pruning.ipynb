{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_pruning as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from archs import resnet9\n",
    "from data_generic import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet9()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"/home/mateuszpyla/stan/sharpness\"\n",
    "if not \"RESULTS\" in os.environ:\n",
    "    os.environ[\"RESULTS\"] = os.path.join(main_dir, \"results\")\n",
    "if not \"DATASETS\" in os.environ:\n",
    "    os.environ[\"DATASETS\"] = os.path.join(main_dir, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR10 dataset\n",
    "train_dataset = CIFAR10(root=os.environ[\"DATASETS\"], train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = CIFAR10(root=os.environ[\"DATASETS\"], train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 256\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_dl = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs_losses, outputs_accs = [], []\n",
    "    for batch in val_loader:\n",
    "        images, labels = batch \n",
    "        out = model(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = get_accuracy(out, labels)           # Calculate accuracy\n",
    "        outputs_losses.append(loss.detach())\n",
    "        outputs_accs.append(acc)\n",
    "    epoch_loss = torch.stack(outputs_losses).mean()   # Combine losses\n",
    "    epoch_acc = torch.stack(outputs_accs).mean()      # Combine accuracies\n",
    "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.12\n",
    "weight_decay = 0.0001\n",
    "opt_func = torch.optim.Adam(model.parameters(),max_lr,amsgrad=True\n",
    "                            , weight_decay=weight_decay)\n",
    "loss_fn = \"mse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\n",
      "[0/196]\n",
      "0.0%\n",
      "Loss: 0.9260492324829102\n",
      "Accuracy: 0.70703125\n",
      "Epoch [{}]\n",
      "train_loss: 0.9260492324829102\n",
      "train_acc: 0.70703125\n",
      "val_loss: 0.892481803894043\n",
      "val_acc: 0.6846449971199036\n",
      "last_lr: [0.0003999999999999993]\n",
      "Train Epoch: 2\n",
      "[0/196]\n",
      "0.0%\n",
      "Loss: 0.8973379135131836\n",
      "Accuracy: 0.69140625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# print(f\"Train Epoch: {epoch+1} [{batch_idx}/{len(train_dl)} ({100. * batch_idx / len(train_dl)}%)]\\tLoss: {loss.item():.6f}, Accuracy: {accuracy.item():.4f}\")\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(train_losses)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     39\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(train_accuracy)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, val_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch \n\u001b[0;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m                    \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out, labels)   \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     acc \u001b[38;5;241m=\u001b[39m get_accuracy(out, labels)           \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/stan/sharpness/src/resnet_cifar.py:326\u001b[0m, in \u001b[0;36mResNet9.forward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xb):\n\u001b[1;32m    325\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(xb)\n\u001b[0;32m--> 326\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres1(out) \u001b[38;5;241m+\u001b[39m out\n\u001b[1;32m    328\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = []\n",
    "optimizer = opt_func\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                            steps_per_epoch=len(train_dl))\n",
    "for epoch in (range(epochs)):\n",
    "    # Training Phase \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracy= []\n",
    "    lrs=[]\n",
    "    for (batch_idx, batch) in enumerate(train_dl):\n",
    "        X, y = batch\n",
    "        out = model(X)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        accuracy = get_accuracy(out, y)\n",
    "        train_losses.append(loss)\n",
    "        train_accuracy.append(accuracy)\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        if grad_clip: \n",
    "            nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Record & update learning rate\n",
    "        lrs.append(get_lr(optimizer))\n",
    "        sched.step()\n",
    "        if batch_idx % 60 == 0:\n",
    "            print(f\"Train Epoch: {epoch+1}\")\n",
    "            print(f\"[{batch_idx}/{len(train_dl)}]\")\n",
    "            print(f\"{100. * batch_idx / len(train_dl)}%\")\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            print(f\"Accuracy: {accuracy.item()}\")\n",
    "            break\n",
    "            # print(f\"Train Epoch: {epoch+1} [{batch_idx}/{len(train_dl)} ({100. * batch_idx / len(train_dl)}%)]\\tLoss: {loss.item():.6f}, Accuracy: {accuracy.item():.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    result = evaluate(model, valid_dl)\n",
    "    result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "    result['train_accuracy'] = torch.stack(train_accuracy).mean().item()\n",
    "    result['lrs'] = lrs\n",
    "    print(\"Epoch [{}]\")\n",
    "    print(f\"train_loss: {result['train_loss']}\")\n",
    "    print(f\"train_acc: {result['train_accuracy']}\")\n",
    "    print(f\"val_loss: {result['val_loss']}\")\n",
    "    print(f\"val_acc: {result['val_acc']}\")\n",
    "    print(f\"last_lr: {result['lrs']}\")\n",
    "    history.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet9:\n\tsize mismatch for conv1.0.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 3, 3, 3]).\n\tsize mismatch for conv1.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv1.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv1.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv1.1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv1.1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv2.0.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for conv2.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv2.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv2.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv2.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv2.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.0.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for res1.0.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.0.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.0.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.0.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.0.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.1.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for res1.1.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.1.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.1.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.1.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.1.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv3.0.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for conv3.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv3.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv3.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv3.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv3.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv4.0.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for conv4.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv4.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv4.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv4.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv4.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.0.0.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for res2.0.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.0.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.0.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.0.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.0.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.1.0.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for res2.1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.1.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.1.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.1.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.1.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for classifier.3.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([0, 0]).\n\tsize mismatch for classifier.3.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for last.3.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([0, 0]).\n\tsize mismatch for last.3.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for gradcam.0.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for gradcam.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for gradcam.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for gradcam.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for gradcam.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for gradcam.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/mateuszpyla/stan/sharpness/results/trained_resnet9.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet9:\n\tsize mismatch for conv1.0.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 3, 3, 3]).\n\tsize mismatch for conv1.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv1.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv1.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv1.1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv1.1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv2.0.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for conv2.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv2.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv2.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv2.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv2.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.0.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for res1.0.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.0.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.0.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.0.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.0.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.1.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for res1.1.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.1.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.1.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.1.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res1.1.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv3.0.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for conv3.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv3.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv3.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv3.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv3.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv4.0.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for conv4.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv4.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv4.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv4.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for conv4.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.0.0.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for res2.0.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.0.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.0.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.0.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.0.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.1.0.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for res2.1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.1.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.1.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.1.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for res2.1.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for classifier.3.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([0, 0]).\n\tsize mismatch for classifier.3.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for last.3.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([0, 0]).\n\tsize mismatch for last.3.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for gradcam.0.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([0, 0, 3, 3]).\n\tsize mismatch for gradcam.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for gradcam.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for gradcam.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for gradcam.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0]).\n\tsize mismatch for gradcam.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([0])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/home/mateuszpyla/stan/sharpness/results/trained_resnet9.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_main = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/mateuszpyla/stan/sharpness/results/trained_resnet9.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "ex = torch.randn(1,3,32,32)\n",
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_prunings(model_main, num_prunings):\n",
    "    amount = random.uniform(0.1, 0.8)\n",
    "    for i in range(num_prunings):\n",
    "        model = copy.deepcopy(model_main)\n",
    "        print(\"prunning \", i)\n",
    "        for group in DG.get_all_groups(ignored_layers=[model.conv1], root_module_types=[nn.Conv2d, nn.Linear]):\n",
    "            group.prune()\n",
    "            print(group)\n",
    "\n",
    "            print(evaluate(model, valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prunning  0\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on classifier.3 (Linear(in_features=512, out_features=0, bias=True)) => prune_out_channels on classifier.3 (Linear(in_features=512, out_features=0, bias=True)), #idxs=10\n",
      "--------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv4.0 (Conv2d(256, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv4.0 (Conv2d(256, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "[1] prune_out_channels on conv4.0 (Conv2d(256, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv4.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[2] prune_out_channels on conv4.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_6(ReluBackward0), #idxs=512\n",
      "[3] prune_out_channels on _ElementWiseOp_6(ReluBackward0) => prune_out_channels on _ElementWiseOp_5(MaxPool2DWithIndicesBackward0), #idxs=512\n",
      "[4] prune_out_channels on _ElementWiseOp_5(MaxPool2DWithIndicesBackward0) => prune_out_channels on _ElementWiseOp_3(AddBackward0), #idxs=512\n",
      "[5] prune_out_channels on _ElementWiseOp_5(MaxPool2DWithIndicesBackward0) => prune_in_channels on res2.0.0 (Conv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "[6] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on _ElementWiseOp_4(ReluBackward0), #idxs=512\n",
      "[7] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on _ElementWiseOp_2(AdaptiveMaxPool2DBackward0), #idxs=512\n",
      "[8] prune_out_channels on _ElementWiseOp_2(AdaptiveMaxPool2DBackward0) => prune_out_channels on _Reshape_0(), #idxs=512\n",
      "[9] prune_out_channels on _Reshape_0() => prune_in_channels on classifier.3 (Linear(in_features=0, out_features=0, bias=True)), #idxs=512\n",
      "[10] prune_in_channels on classifier.3 (Linear(in_features=0, out_features=0, bias=True)) => prune_out_channels on _ElementWiseOp_1(TBackward0), #idxs=512\n",
      "[11] prune_out_channels on _ElementWiseOp_4(ReluBackward0) => prune_out_channels on res2.1.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[12] prune_out_channels on res2.1.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on res2.1.0 (Conv2d(512, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv3.0 (Conv2d(128, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv3.0 (Conv2d(128, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=256\n",
      "[1] prune_out_channels on conv3.0 (Conv2d(128, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv3.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=256\n",
      "[2] prune_out_channels on conv3.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_8(ReluBackward0), #idxs=256\n",
      "[3] prune_out_channels on _ElementWiseOp_8(ReluBackward0) => prune_out_channels on _ElementWiseOp_7(MaxPool2DWithIndicesBackward0), #idxs=256\n",
      "[4] prune_out_channels on _ElementWiseOp_7(MaxPool2DWithIndicesBackward0) => prune_in_channels on conv4.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=256\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv2.0 (Conv2d(64, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv2.0 (Conv2d(64, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[1] prune_out_channels on conv2.0 (Conv2d(64, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv2.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[2] prune_out_channels on conv2.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_12(ReluBackward0), #idxs=128\n",
      "[3] prune_out_channels on _ElementWiseOp_12(ReluBackward0) => prune_out_channels on _ElementWiseOp_11(MaxPool2DWithIndicesBackward0), #idxs=128\n",
      "[4] prune_out_channels on _ElementWiseOp_11(MaxPool2DWithIndicesBackward0) => prune_out_channels on _ElementWiseOp_9(AddBackward0), #idxs=128\n",
      "[5] prune_out_channels on _ElementWiseOp_11(MaxPool2DWithIndicesBackward0) => prune_in_channels on res1.0.0 (Conv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[6] prune_out_channels on _ElementWiseOp_9(AddBackward0) => prune_out_channels on _ElementWiseOp_10(ReluBackward0), #idxs=128\n",
      "[7] prune_out_channels on _ElementWiseOp_9(AddBackward0) => prune_in_channels on conv3.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[8] prune_out_channels on _ElementWiseOp_10(ReluBackward0) => prune_out_channels on res1.1.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[9] prune_out_channels on res1.1.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on res1.1.0 (Conv2d(128, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv1.0 (Conv2d(3, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv1.0 (Conv2d(3, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=64\n",
      "[1] prune_out_channels on conv1.0 (Conv2d(3, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv1.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=64\n",
      "[2] prune_out_channels on conv1.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_13(ReluBackward0), #idxs=64\n",
      "[3] prune_out_channels on _ElementWiseOp_13(ReluBackward0) => prune_in_channels on conv2.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=64\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on res1.0.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res1.0.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[1] prune_out_channels on res1.0.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res1.0.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[2] prune_out_channels on res1.0.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_14(ReluBackward0), #idxs=128\n",
      "[3] prune_out_channels on _ElementWiseOp_14(ReluBackward0) => prune_in_channels on res1.1.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on res2.0.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res2.0.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "[1] prune_out_channels on res2.0.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res2.0.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[2] prune_out_channels on res2.0.1 (BatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_15(ReluBackward0), #idxs=512\n",
      "[3] prune_out_channels on _ElementWiseOp_15(ReluBackward0) => prune_in_channels on res2.1.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "prunning  1\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on classifier.3 (Linear(in_features=0, out_features=0, bias=True)) => prune_out_channels on classifier.3 (Linear(in_features=0, out_features=0, bias=True)), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv4.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv4.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv3.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv3.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv2.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv2.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv1.0 (Conv2d(3, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv1.0 (Conv2d(3, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on res1.1.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res1.1.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on res1.0.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res1.0.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on res2.1.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res2.1.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on res2.0.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res2.0.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "prunning  2\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on classifier.3 (Linear(in_features=0, out_features=0, bias=True)) => prune_out_channels on classifier.3 (Linear(in_features=0, out_features=0, bias=True)), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv4.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv4.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv3.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv3.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n",
      "{'val_loss': 0.893187403678894, 'val_acc': 0.6840475797653198}\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv2.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv2.0 (Conv2d(0, 0, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=0\n",
      "--------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrandom_prunings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_main\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mrandom_prunings\u001b[0;34m(model_main, num_prunings)\u001b[0m\n\u001b[1;32m      7\u001b[0m group\u001b[38;5;241m.\u001b[39mprune()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(group)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, val_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch \n\u001b[0;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m                    \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out, labels)   \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     acc \u001b[38;5;241m=\u001b[39m get_accuracy(out, labels)           \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/stan/sharpness/src/resnet_cifar.py:326\u001b[0m, in \u001b[0;36mResNet9.forward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xb):\n\u001b[1;32m    325\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(xb)\n\u001b[0;32m--> 326\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres1(out) \u001b[38;5;241m+\u001b[39m out\n\u001b[1;32m    328\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_prunings(model_main, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySlimmingPruner(tp.pruner.MetaPruner):\n",
    "    def regularize(self, model, reg):\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)) and m.affine==True:\n",
    "                m.weight.grad.data.add_(reg*torch.sign(m.weight.data)) # Lasso for sparsity\n",
    "\n",
    "class MySlimmingImportance(tp.importance.Importance):\n",
    "    def __call__(self, group, **kwargs):\n",
    "        #note that we have multiple BNs in a group, \n",
    "        # we store layer-wise scores in a list and then reduce them to get the final results\n",
    "        group_imp = [] # (num_bns, num_channels) \n",
    "        # 1. iterate the group to estimate importance\n",
    "        for dep, idxs in group:\n",
    "            layer = dep.target.module # get the target model\n",
    "            prune_fn = dep.handler    # get the pruning function of target model, unused in this example\n",
    "            if isinstance(layer, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)) and layer.affine:\n",
    "                local_imp = torch.abs(layer.weight.data)\n",
    "                group_imp.append(local_imp)\n",
    "        if len(group_imp)==0: return None # return None if the group contains no BN layer\n",
    "        # 2. reduce your group importance to a 1-D scroe vector. Here we use the average score across layers.\n",
    "        group_imp = torch.stack(group_imp, dim=0).mean(dim=0) \n",
    "        return group_imp # (num_channels, )\n",
    "\n",
    "# You can implement any importance functions, as long as it transforms a group to a 1-D score vector.\n",
    "class RandomImportance(tp.importance.Importance):\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, group, **kwargs):\n",
    "        _, idxs = group[0]\n",
    "        return torch.rand(len(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, expected weight to be at least 1 at dimension 0, but got weight of size [0, 3, 3, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch_pruning/dependency.py:778\u001b[0m, in \u001b[0;36mDependencyGraph._trace\u001b[0;34m(self, model, example_inputs, forward_fn, output_transform)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 778\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m~/stan/sharpness/src/resnet_cifar.py:325\u001b[0m, in \u001b[0;36mResNet9.forward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xb):\n\u001b[0;32m--> 325\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, expected weight to be at least 1 at dimension 0, but got weight of size [0, 3, 3, 3] instead",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         ignored_layers\u001b[38;5;241m.\u001b[39mappend(m)\n\u001b[1;32m      8\u001b[0m iterative_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 9\u001b[0m pruner \u001b[38;5;241m=\u001b[39m \u001b[43mMySlimmingPruner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_pruning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimportance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterative_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterative_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpruning_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignored_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignored_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch_pruning/pruner/algorithms/metapruner.py:131\u001b[0m, in \u001b[0;36mMetaPruner.__init__\u001b[0;34m(self, model, example_inputs, importance, global_pruning, pruning_ratio, pruning_ratio_dict, max_pruning_ratio, iterative_steps, iterative_pruning_ratio_scheduler, ignored_layers, round_to, in_channel_groups, out_channel_groups, num_heads, prune_num_heads, prune_head_dims, head_pruning_ratio, head_pruning_ratio_dict, customized_pruners, unwrapped_parameters, root_module_types, forward_fn, output_transform, channel_groups, ch_sparsity, ch_sparsity_dict)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignored_params\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m###############################################\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Build dependency graph\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDG \u001b[38;5;241m=\u001b[39m \u001b[43mdependency\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDependencyGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_dependency\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43munwrapped_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munwrapped_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustomized_pruners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustomized_pruners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignored_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignored_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m###############################################\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Iterative pruning\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# The pruner will prune the model iteratively for several steps to achieve the target pruning ratio\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# E.g., if iterative_steps=5, pruning_ratio=0.5, the pruning ratio of each step will be [0.1, 0.2, 0.3, 0.4, 0.5]\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterative_steps \u001b[38;5;241m=\u001b[39m iterative_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch_pruning/dependency.py:382\u001b[0m, in \u001b[0;36mDependencyGraph.build_dependency\u001b[0;34m(self, model, example_inputs, forward_fn, output_transform, unwrapped_parameters, customized_pruners, ignored_layers, ignored_params, verbose)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDependency graph relies on autograd for tracing. Please check and disable the torch.no_grad() in your code.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Build computational graph through tracing. \u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule2node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_transform\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Build dependency graph\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_dependency(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule2node)\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch_pruning/dependency.py:780\u001b[0m, in \u001b[0;36mDependencyGraph._trace\u001b[0;34m(self, model, example_inputs, forward_fn, output_transform)\u001b[0m\n\u001b[1;32m    778\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39mexample_inputs)\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 780\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n\u001b[1;32m    782\u001b[0m     hook\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/stan/sharpness/src/resnet_cifar.py:325\u001b[0m, in \u001b[0;36mResNet9.forward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xb):\n\u001b[0;32m--> 325\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[1;32m    327\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres1(out) \u001b[38;5;241m+\u001b[39m out\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, expected weight to be at least 1 at dimension 0, but got weight of size [0, 3, 3, 3] instead"
     ]
    }
   ],
   "source": [
    "imp = MySlimmingImportance()\n",
    "\n",
    "ignored_layers = []\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear) and m.out_features == 10:\n",
    "        ignored_layers.append(m)\n",
    "\n",
    "iterative_steps = 5\n",
    "pruner = MySlimmingPruner(\n",
    "    model, \n",
    "    ex, \n",
    "    global_pruning=False,\n",
    "    importance=imp,\n",
    "    iterative_steps=iterative_steps,\n",
    "    pruning_ratio=0.5,\n",
    "    ignored_layers=ignored_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: variables __flops__ or __params__ are already defined for the moduleConv2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleBatchNorm2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleReLU ptflops can affect your code!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, expected weight to be at least 1 at dimension 0, but got weight of size [0, 3, 3, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_macs, base_nparams \u001b[38;5;241m=\u001b[39m \u001b[43mtp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_ops_and_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterative_steps):\n\u001b[1;32m      3\u001b[0m     pruner\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch_pruning/utils/op_counter.py:35\u001b[0m, in \u001b[0;36mcount_ops_and_params\u001b[0;34m(model, example_inputs, layer_wise)\u001b[0m\n\u001b[1;32m     33\u001b[0m     _ \u001b[38;5;241m=\u001b[39m flops_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample_inputs)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mflops_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m flops_count, params_count, _layer_flops, _layer_params \u001b[38;5;241m=\u001b[39m flops_model\u001b[38;5;241m.\u001b[39mcompute_average_flops_cost()\n\u001b[1;32m     37\u001b[0m layer_flops \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/stan/sharpness/src/resnet_cifar.py:325\u001b[0m, in \u001b[0;36mResNet9.forward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xb):\n\u001b[0;32m--> 325\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[1;32m    327\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres1(out) \u001b[38;5;241m+\u001b[39m out\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bfn/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, expected weight to be at least 1 at dimension 0, but got weight of size [0, 3, 3, 3] instead"
     ]
    }
   ],
   "source": [
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model, ex)\n",
    "for i in range(iterative_steps):\n",
    "    pruner.step()\n",
    "\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, ex)\n",
    "    print(model)\n",
    "    print(model(ex).shape)\n",
    "    print(\n",
    "        \"  Iter %d/%d, Params: %.2f M => %.2f M\"\n",
    "        % (i+1, iterative_steps, base_nparams / 1e6, nparams / 1e6)\n",
    "    )\n",
    "    print(\n",
    "        \"  Iter %d/%d, MACs: %.2f G => %.2f G\"\n",
    "        % (i+1, iterative_steps, base_macs / 1e9, macs / 1e9)\n",
    "    )\n",
    "    print(\"=\"*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for x in model._modules.keys():\n",
    "            layer = getattr(model, x)\n",
    "            group = DG.get_pruning_group(layer, tp.prune_conv_out_channels, idxs=[2, 6, 9] )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def random_prunings(model_main, num_prunings):\n",
    "    amount = random.uniform(0.1, 0.8)\n",
    "    # method = random.choice(['magnitude', 'random_unstructured'])\n",
    "    imp = tp.importance.TaylorImportance()\n",
    "\n",
    "    selected_layers = list(model._modules.keys()) # random.sample\n",
    "\n",
    "    for _ in range(num_prunings):\n",
    "        pruner = tp.pruner.MagnitudePruner(\n",
    "            model,\n",
    "            ex,\n",
    "            importance=imp,\n",
    "            iterative_steps=5,\n",
    "            ch_sparsity=amount\n",
    "        )\n",
    "\n",
    "        base_macs, base_nparams = tp.utils.count_ops_and_params(model, ex)\n",
    "        for layer_name in selected_layers:\n",
    "        for i in range(iterative_steps):\n",
    "            if isinstance(imp, tp.importance.TaylorImportance):\n",
    "            pruner.prune(layer, method=pruning_method, amount=amount)\n",
    "        macs, nparams = tp.utils.count_ops_and_params(model, example_inputs) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on classifier.3 (Linear(in_features=512, out_features=10, bias=True)) => prune_out_channels on classifier.3 (Linear(in_features=512, out_features=10, bias=True)), #idxs=10\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv4.0 (Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv4.0 (Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "[1] prune_out_channels on conv4.0 (Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv4.1 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[2] prune_out_channels on conv4.1 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_6(ReluBackward0), #idxs=512\n",
      "[3] prune_out_channels on _ElementWiseOp_6(ReluBackward0) => prune_out_channels on _ElementWiseOp_5(MaxPool2DWithIndicesBackward0), #idxs=512\n",
      "[4] prune_out_channels on _ElementWiseOp_5(MaxPool2DWithIndicesBackward0) => prune_out_channels on _ElementWiseOp_3(AddBackward0), #idxs=512\n",
      "[5] prune_out_channels on _ElementWiseOp_5(MaxPool2DWithIndicesBackward0) => prune_in_channels on res2.0.0 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "[6] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on _ElementWiseOp_4(ReluBackward0), #idxs=512\n",
      "[7] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on _ElementWiseOp_2(AdaptiveMaxPool2DBackward0), #idxs=512\n",
      "[8] prune_out_channels on _ElementWiseOp_2(AdaptiveMaxPool2DBackward0) => prune_out_channels on _Reshape_0(), #idxs=512\n",
      "[9] prune_out_channels on _Reshape_0() => prune_in_channels on classifier.3 (Linear(in_features=512, out_features=10, bias=True)), #idxs=512\n",
      "[10] prune_in_channels on classifier.3 (Linear(in_features=512, out_features=10, bias=True)) => prune_out_channels on _ElementWiseOp_1(TBackward0), #idxs=512\n",
      "[11] prune_out_channels on _ElementWiseOp_4(ReluBackward0) => prune_out_channels on res2.1.1 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[12] prune_out_channels on res2.1.1 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on res2.1.0 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv3.0 (Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv3.0 (Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=256\n",
      "[1] prune_out_channels on conv3.0 (Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv3.1 (BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=256\n",
      "[2] prune_out_channels on conv3.1 (BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_8(ReluBackward0), #idxs=256\n",
      "[3] prune_out_channels on _ElementWiseOp_8(ReluBackward0) => prune_out_channels on _ElementWiseOp_7(MaxPool2DWithIndicesBackward0), #idxs=256\n",
      "[4] prune_out_channels on _ElementWiseOp_7(MaxPool2DWithIndicesBackward0) => prune_in_channels on conv4.0 (Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=256\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv2.0 (Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv2.0 (Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[1] prune_out_channels on conv2.0 (Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv2.1 (BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[2] prune_out_channels on conv2.1 (BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_12(ReluBackward0), #idxs=128\n",
      "[3] prune_out_channels on _ElementWiseOp_12(ReluBackward0) => prune_out_channels on _ElementWiseOp_11(MaxPool2DWithIndicesBackward0), #idxs=128\n",
      "[4] prune_out_channels on _ElementWiseOp_11(MaxPool2DWithIndicesBackward0) => prune_out_channels on _ElementWiseOp_9(AddBackward0), #idxs=128\n",
      "[5] prune_out_channels on _ElementWiseOp_11(MaxPool2DWithIndicesBackward0) => prune_in_channels on res1.0.0 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[6] prune_out_channels on _ElementWiseOp_9(AddBackward0) => prune_out_channels on _ElementWiseOp_10(ReluBackward0), #idxs=128\n",
      "[7] prune_out_channels on _ElementWiseOp_9(AddBackward0) => prune_in_channels on conv3.0 (Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[8] prune_out_channels on _ElementWiseOp_10(ReluBackward0) => prune_out_channels on res1.1.1 (BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[9] prune_out_channels on res1.1.1 (BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on res1.1.0 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on conv1.0 (Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv1.0 (Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=64\n",
      "[1] prune_out_channels on conv1.0 (Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on conv1.1 (BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=64\n",
      "[2] prune_out_channels on conv1.1 (BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_13(ReluBackward0), #idxs=64\n",
      "[3] prune_out_channels on _ElementWiseOp_13(ReluBackward0) => prune_in_channels on conv2.0 (Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=64\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on res1.0.0 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res1.0.0 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "[1] prune_out_channels on res1.0.0 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res1.0.1 (BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[2] prune_out_channels on res1.0.1 (BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_14(ReluBackward0), #idxs=128\n",
      "[3] prune_out_channels on _ElementWiseOp_14(ReluBackward0) => prune_in_channels on res1.1.0 (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=128\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on res2.0.0 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res2.0.0 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "[1] prune_out_channels on res2.0.0 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))) => prune_out_channels on res2.0.1 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[2] prune_out_channels on res2.0.1 (BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_15(ReluBackward0), #idxs=512\n",
      "[3] prune_out_channels on _ElementWiseOp_15(ReluBackward0) => prune_in_channels on res2.1.0 (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), #idxs=512\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for group in DG.get_all_groups(ignored_layers=[], root_module_types=[nn.Conv2d, nn.Linear]):\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ConvPruner.prune_out_channels of <torch_pruning.pruner.function.ConvPruner object at 0x7f86ac55ba90>>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.prune_conv_out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[2, 6, 9] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_dataset(\"coloured_mnist_split1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_steps):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
