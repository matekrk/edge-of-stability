{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import SVHN\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from data_fashion import FashionMNIST, FASHION_LABELS\n",
    "from data_svhn import SVHN_LABELS\n",
    "from data_generic import load_dataset\n",
    "from utilities import iterate_dataset, get_loss_and_acc, get_gd_optimizer, compute_losses, get_hessian_eigenvalues\n",
    "from archs import load_architecture\n",
    "\n",
    "from data_coloured_mnist import AugmentTensorDataset, cifar_transform, flatten, make_labels, standardize, unflatten\n",
    "from data_cifar_c import load_cifar_corrupted, CIFAR10C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = str(Path().resolve().parent)\n",
    "if not \"RESULTS\" in os.environ:\n",
    "    os.environ[\"RESULTS\"] = os.path.join(main_dir, \"results\")\n",
    "    results_dir = os.environ[\"RESULTS\"]\n",
    "if not \"DATASETS\" in os.environ:\n",
    "    os.environ[\"DATASETS\"] = os.path.join(main_dir, \"data\")\n",
    "    data_dir = os.environ[\"DATASETS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = CIFAR10(data_dir)\n",
    "dt = CIFAR10(data_dir, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/mateuszpyla/stan/sharpness/data/train_32x32.mat\n",
      "Using downloaded and verified file: /home/mateuszpyla/stan/sharpness/data/train_32x32.mat\n",
      "Using downloaded and verified file: /home/mateuszpyla/stan/sharpness/data/extra_32x32.mat\n",
      "Using downloaded and verified file: /home/mateuszpyla/stan/sharpness/data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "svhn_train = SVHN(root=data_dir, download=True, split=\"train\")\n",
    "\n",
    "dataset_name = \"svhn\"\n",
    "loss = \"mse\"\n",
    "svhn_train_dataset, svhn_test_dataset = load_dataset(dataset_name, loss)\n",
    "physical_batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/mateuszpyla/stan/sharpness/data/train_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "svhn_train = SVHN(root=data_dir, download=True, split=\"train\")\n",
    "\n",
    "dataset_name = \"cifar10\"\n",
    "loss = \"mse\"\n",
    "cifar10_train_dataset, cifar10_test_dataset = load_dataset(dataset_name, loss)\n",
    "physical_batch_size = 1000\n",
    "cutted_dataset_name = \"cifar10-5k\"\n",
    "cifar10_train_cutted_dataset = load_dataset(cutted_dataset_name, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_id = \"resnet9\"\n",
    "dynamic = False\n",
    "network = load_architecture(arch_id, dataset_name, dynamic).cuda()\n",
    "\n",
    "loss_fn, acc_fn = get_loss_and_acc(loss)\n",
    "loss_fn.__setattr__(\"individual\", False)\n",
    "acc_fn.__setattr__(\"individual\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = filter(lambda p: p.requires_grad, network.parameters())\n",
    "opt = \"sgd\"\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "momentum = 0.0\n",
    "\n",
    "optimizer = get_gd_optimizer(params, opt, lr, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_s, train_acc_s, test_loss_s, test_acc_s = [], [], [], []\n",
    "eigs = []\n",
    "networks = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, batch in enumerate(iterate_dataset(cifar10_train_dataset, physical_batch_size)):\n",
    "        print(f\"epoch {e} iter {i}\")\n",
    "        (X, y) = batch\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        \n",
    "        loss = loss_fn(network(X), y) / len(X)\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss, train_acc = compute_losses(network, [loss_fn, acc_fn], cifar10_train_dataset, physical_batch_size)\n",
    "        test_loss, test_acc = compute_losses(network, [loss_fn, acc_fn], cifar10_test_dataset, physical_batch_size)\n",
    "        current_eigs = get_hessian_eigenvalues(network, loss_fn, cifar10_train_cutted_dataset, neigs=2, physical_batch_size=100)\n",
    "\n",
    "        train_loss_s.append(train_loss)\n",
    "        train_acc_s.append(train_acc)\n",
    "        test_loss_s.append(test_loss)\n",
    "        test_acc_s.append(test_acc)\n",
    "        eigs.append(current_eigs)\n",
    "        networks.append(copy.deepcopy(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(train_loss_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_id = 10\n",
    "network_checkpoint = networks[checkpoint_id]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
